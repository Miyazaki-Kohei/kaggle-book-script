{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-17T16:16:10.791057Z","iopub.execute_input":"2024-06-17T16:16:10.791532Z","iopub.status.idle":"2024-06-17T16:16:10.803325Z","shell.execute_reply.started":"2024-06-17T16:16:10.791492Z","shell.execute_reply":"2024-06-17T16:16:10.802199Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"/kaggle/input/porto-seguro-safe-driver-prediction/sample_submission.csv\n/kaggle/input/porto-seguro-safe-driver-prediction/train.csv\n/kaggle/input/porto-seguro-safe-driver-prediction/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport optuna\nimport lightgbm as lgb\nfrom path import Path\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:16:10.805335Z","iopub.execute_input":"2024-06-17T16:16:10.806040Z","iopub.status.idle":"2024-06-17T16:16:10.816779Z","shell.execute_reply.started":"2024-06-17T16:16:10.805996Z","shell.execute_reply":"2024-06-17T16:16:10.815574Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"\nclass Config:\n    input_path = Path('../input/porto-seguro-safe-driver-prediction')\n    optuna_lgb = False\n    n_estimators = 1500\n    early_stopping_round = 150\n    cv_folds = 5\n    random_state = 0\n    params = {'objective': 'binary',\n              'boosting_type': 'gbdt',\n              'learning_rate': 0.01,\n              'max_bin': 25,\n              'num_leaves': 31,\n              'min_child_samples': 1500,\n              'colsample_bytree': 0.7,\n              'subsample_freq': 1,\n              'subsample': 0.7,\n              'reg_alpha': 1.0,\n              'reg_lambda': 1.0,\n              'verbosity': 0,\n              'random_state': 0}\n    \nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:16:10.818846Z","iopub.execute_input":"2024-06-17T16:16:10.819326Z","iopub.status.idle":"2024-06-17T16:16:10.831442Z","shell.execute_reply.started":"2024-06-17T16:16:10.819285Z","shell.execute_reply":"2024-06-17T16:16:10.829860Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(config.input_path / 'train.csv', index_col='id')\ntest = pd.read_csv(config.input_path / 'test.csv', index_col='id')\nsubmission = pd.read_csv(config.input_path / 'sample_submission.csv', index_col='id')\n# train[\"target\"].unique()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:16:10.833347Z","iopub.execute_input":"2024-06-17T16:16:10.833714Z","iopub.status.idle":"2024-06-17T16:16:18.049594Z","shell.execute_reply.started":"2024-06-17T16:16:10.833684Z","shell.execute_reply":"2024-06-17T16:16:18.048536Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"calc_features = [feat for feat in train.columns if \"_calc\" in feat]\ncat_features = [feat for feat in train.columns if \"_cat\" in feat]\n# cat_features","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:16:18.050793Z","iopub.execute_input":"2024-06-17T16:16:18.051129Z","iopub.status.idle":"2024-06-17T16:16:18.056728Z","shell.execute_reply.started":"2024-06-17T16:16:18.051103Z","shell.execute_reply":"2024-06-17T16:16:18.055568Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"target = train[\"target\"]\ntrain = train.drop(\"target\", axis=\"columns\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:16:18.059016Z","iopub.execute_input":"2024-06-17T16:16:18.059388Z","iopub.status.idle":"2024-06-17T16:16:18.151649Z","shell.execute_reply.started":"2024-06-17T16:16:18.059353Z","shell.execute_reply":"2024-06-17T16:16:18.150621Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"train = train.drop(calc_features, axis=\"columns\")\ntest = test.drop(calc_features, axis=\"columns\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:16:18.152897Z","iopub.execute_input":"2024-06-17T16:16:18.153217Z","iopub.status.idle":"2024-06-17T16:16:18.272588Z","shell.execute_reply.started":"2024-06-17T16:16:18.153191Z","shell.execute_reply":"2024-06-17T16:16:18.271332Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"train = pd.get_dummies(train, columns=cat_features, dtype=float)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:16:18.273917Z","iopub.execute_input":"2024-06-17T16:16:18.274251Z","iopub.status.idle":"2024-06-17T16:16:19.530704Z","shell.execute_reply.started":"2024-06-17T16:16:18.274222Z","shell.execute_reply":"2024-06-17T16:16:19.529497Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"test = pd.get_dummies(test, columns=cat_features, dtype=float)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:16:19.534690Z","iopub.execute_input":"2024-06-17T16:16:19.535223Z","iopub.status.idle":"2024-06-17T16:16:21.386426Z","shell.execute_reply.started":"2024-06-17T16:16:19.535186Z","shell.execute_reply":"2024-06-17T16:16:21.385314Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"assert((train.columns==test.columns).all())","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:16:21.387784Z","iopub.execute_input":"2024-06-17T16:16:21.388121Z","iopub.status.idle":"2024-06-17T16:16:21.395383Z","shell.execute_reply.started":"2024-06-17T16:16:21.388092Z","shell.execute_reply":"2024-06-17T16:16:21.394467Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"from numba import jit\n\n@jit\ndef eval_gini(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_pred)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini\n\ndef gini_lgb(y_true, y_pred):\n    eval_name = 'normalized_gini_coef'\n    eval_result = eval_gini(y_true, y_pred)\n    is_higher_better = True\n    return eval_name, eval_result, is_higher_better","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:18:21.496248Z","iopub.execute_input":"2024-06-17T16:18:21.496732Z","iopub.status.idle":"2024-06-17T16:18:21.505988Z","shell.execute_reply.started":"2024-06-17T16:18:21.496698Z","shell.execute_reply":"2024-06-17T16:18:21.504577Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/2566664522.py:4: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n  def eval_gini(y_true, y_pred):\n","output_type":"stream"}]},{"cell_type":"code","source":"import optuna\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\n\nif config.optuna_lgb:\n    \n    def objective(trial):\n        params = {\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 1.0),\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 3, 255),\n            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 3, 3000),\n            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n            \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 0, 10),\n            \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1.0),\n            \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-9, 10.0),\n            \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-9, 10.0)\n        }\n        \n        score = list()\n        skf = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, random_state=config.random_state)\n        \n        for train_idx, valid_idx in skf.split(train, target):\n            X_train = train.iloc[train_idx]\n            y_train = target.iloc[train_idx]\n            X_valid = train.iloc[valid_idx]\n            y_valid = target.iloc[valid_idx]\n            \n            model = lgb.LGBMClassifier(**params, n_estimators=1500, early_stopping_round=150, force_row_wise=True)\n            \n            callbacks = [lgb.early_stopping(stopping_rounds=150, verbose=False)]\n            \n            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric=gini_lgb, callbacks=callbacks)\n            \n            score.append(model.best_score_[\"valid_0\"][\"normalized_gini_coef\"])\n            \n        return np.mean(score)\n    \n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=300)\n    \n    print(\"Best Gini Normalized Score\", study.best_value)\n    print(\"Best parameters\", study.best_params)\n    \n    params = {\n        \"objective\": \"binary\",\n        \"boosting_type\": \"gbdt\",\n        \"verbosity\": 0,\n        \"random_state\": 0\n    }\n    \n    params.update(study.best_params)\nelse:\n    params = config.params\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:18:23.470826Z","iopub.execute_input":"2024-06-17T16:18:23.471202Z","iopub.status.idle":"2024-06-17T16:18:23.483538Z","shell.execute_reply.started":"2024-06-17T16:18:23.471175Z","shell.execute_reply":"2024-06-17T16:18:23.482228Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"preds = np.zeros(len(test))\noof = np.zeros(len(train))\nmetric_evaluations = list()\n\nskf = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, random_state=config.random_state)\n\nfor idx, (train_idx, valid_idx) in enumerate(skf.split(train, target)):\n    print(f\"CV fold {idx}\")\n    X_train, y_train = train.iloc[train_idx], target.iloc[train_idx]\n    X_valid, y_valid = train.iloc[valid_idx], target.iloc[valid_idx]\n    \n    model = lgb.LGBMClassifier(**params, n_estimators=config.n_estimators, early_stopping_round=config.early_stopping_round, force_row_wise=True)\n    \n    callbacks=[lgb.early_stopping(stopping_rounds=150),\n              lgb.log_evaluation(period=100, show_stdv=False)]\n    \n    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],\n             eval_metric=gini_lgb, callbacks=callbacks)\n    \n    metric_evaluations.append(model.best_score_[\"valid_0\"][\"normalized_gini_coef\"])\n    preds += (model.predict_proba(test, num_iteration=model.best_iteration_)[:, 1] / skf.n_splits)\n    \n    oof[valid_idx] = model.predict_proba(X_valid, num_iteration=model.best_iteration_)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:18:24.226737Z","iopub.execute_input":"2024-06-17T16:18:24.227109Z","iopub.status.idle":"2024-06-17T16:44:43.007000Z","shell.execute_reply.started":"2024-06-17T16:18:24.227083Z","shell.execute_reply":"2024-06-17T16:44:43.005641Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"CV fold 0\nTraining until validation scores don't improve for 150 rounds\n[100]\tvalid_0's binary_logloss: 0.153243\tvalid_0's normalized_gini_coef: 0.271457\n[200]\tvalid_0's binary_logloss: 0.15228\tvalid_0's normalized_gini_coef: 0.280599\n[300]\tvalid_0's binary_logloss: 0.15185\tvalid_0's normalized_gini_coef: 0.286829\n[400]\tvalid_0's binary_logloss: 0.151651\tvalid_0's normalized_gini_coef: 0.289906\n[500]\tvalid_0's binary_logloss: 0.151543\tvalid_0's normalized_gini_coef: 0.291906\n[600]\tvalid_0's binary_logloss: 0.151473\tvalid_0's normalized_gini_coef: 0.293377\n[700]\tvalid_0's binary_logloss: 0.151437\tvalid_0's normalized_gini_coef: 0.293827\n[800]\tvalid_0's binary_logloss: 0.151417\tvalid_0's normalized_gini_coef: 0.294276\n[900]\tvalid_0's binary_logloss: 0.15142\tvalid_0's normalized_gini_coef: 0.294119\nCV fold 1\nTraining until validation scores don't improve for 150 rounds\n[100]\tvalid_0's binary_logloss: 0.153553\tvalid_0's normalized_gini_coef: 0.255568\n[200]\tvalid_0's binary_logloss: 0.152779\tvalid_0's normalized_gini_coef: 0.261176\n[300]\tvalid_0's binary_logloss: 0.152509\tvalid_0's normalized_gini_coef: 0.264598\n[400]\tvalid_0's binary_logloss: 0.152392\tvalid_0's normalized_gini_coef: 0.266942\n[500]\tvalid_0's binary_logloss: 0.152334\tvalid_0's normalized_gini_coef: 0.268508\n[600]\tvalid_0's binary_logloss: 0.15231\tvalid_0's normalized_gini_coef: 0.269259\n[700]\tvalid_0's binary_logloss: 0.152308\tvalid_0's normalized_gini_coef: 0.269299\n[800]\tvalid_0's binary_logloss: 0.1523\tvalid_0's normalized_gini_coef: 0.269814\n[900]\tvalid_0's binary_logloss: 0.152298\tvalid_0's normalized_gini_coef: 0.270119\nEarly stopping, best iteration is:\n[781]\tvalid_0's binary_logloss: 0.152296\tvalid_0's normalized_gini_coef: 0.269945\nCV fold 2\nTraining until validation scores don't improve for 150 rounds\n[100]\tvalid_0's binary_logloss: 0.15349\tvalid_0's normalized_gini_coef: 0.250438\n[200]\tvalid_0's binary_logloss: 0.152638\tvalid_0's normalized_gini_coef: 0.261463\n[300]\tvalid_0's binary_logloss: 0.152286\tvalid_0's normalized_gini_coef: 0.267762\n[400]\tvalid_0's binary_logloss: 0.15211\tvalid_0's normalized_gini_coef: 0.271644\n[500]\tvalid_0's binary_logloss: 0.152015\tvalid_0's normalized_gini_coef: 0.274152\n[600]\tvalid_0's binary_logloss: 0.151963\tvalid_0's normalized_gini_coef: 0.275609\n[700]\tvalid_0's binary_logloss: 0.151933\tvalid_0's normalized_gini_coef: 0.276576\n[800]\tvalid_0's binary_logloss: 0.151919\tvalid_0's normalized_gini_coef: 0.276946\n[900]\tvalid_0's binary_logloss: 0.151906\tvalid_0's normalized_gini_coef: 0.277448\n[1000]\tvalid_0's binary_logloss: 0.151911\tvalid_0's normalized_gini_coef: 0.277363\nCV fold 3\nTraining until validation scores don't improve for 150 rounds\n[100]\tvalid_0's binary_logloss: 0.153151\tvalid_0's normalized_gini_coef: 0.28492\n[200]\tvalid_0's binary_logloss: 0.152081\tvalid_0's normalized_gini_coef: 0.294826\n[300]\tvalid_0's binary_logloss: 0.151594\tvalid_0's normalized_gini_coef: 0.301155\n[400]\tvalid_0's binary_logloss: 0.151332\tvalid_0's normalized_gini_coef: 0.305416\n[500]\tvalid_0's binary_logloss: 0.151173\tvalid_0's normalized_gini_coef: 0.308713\n[600]\tvalid_0's binary_logloss: 0.151074\tvalid_0's normalized_gini_coef: 0.310518\n[700]\tvalid_0's binary_logloss: 0.151014\tvalid_0's normalized_gini_coef: 0.311803\n[800]\tvalid_0's binary_logloss: 0.150976\tvalid_0's normalized_gini_coef: 0.312533\n[900]\tvalid_0's binary_logloss: 0.150947\tvalid_0's normalized_gini_coef: 0.31291\n[1000]\tvalid_0's binary_logloss: 0.150928\tvalid_0's normalized_gini_coef: 0.313239\n[1100]\tvalid_0's binary_logloss: 0.15092\tvalid_0's normalized_gini_coef: 0.313284\n[1200]\tvalid_0's binary_logloss: 0.150921\tvalid_0's normalized_gini_coef: 0.313124\nEarly stopping, best iteration is:\n[1066]\tvalid_0's binary_logloss: 0.150919\tvalid_0's normalized_gini_coef: 0.313463\nCV fold 4\nTraining until validation scores don't improve for 150 rounds\n[100]\tvalid_0's binary_logloss: 0.153417\tvalid_0's normalized_gini_coef: 0.259211\n[200]\tvalid_0's binary_logloss: 0.152528\tvalid_0's normalized_gini_coef: 0.268649\n[300]\tvalid_0's binary_logloss: 0.152123\tvalid_0's normalized_gini_coef: 0.27534\n[400]\tvalid_0's binary_logloss: 0.15192\tvalid_0's normalized_gini_coef: 0.279392\n[500]\tvalid_0's binary_logloss: 0.151813\tvalid_0's normalized_gini_coef: 0.28184\n[600]\tvalid_0's binary_logloss: 0.151733\tvalid_0's normalized_gini_coef: 0.283741\n[700]\tvalid_0's binary_logloss: 0.151682\tvalid_0's normalized_gini_coef: 0.284936\n[800]\tvalid_0's binary_logloss: 0.151647\tvalid_0's normalized_gini_coef: 0.286006\n[900]\tvalid_0's binary_logloss: 0.151625\tvalid_0's normalized_gini_coef: 0.28652\n[1000]\tvalid_0's binary_logloss: 0.151608\tvalid_0's normalized_gini_coef: 0.286903\n[1100]\tvalid_0's binary_logloss: 0.151595\tvalid_0's normalized_gini_coef: 0.287265\n[1200]\tvalid_0's binary_logloss: 0.15159\tvalid_0's normalized_gini_coef: 0.287404\n[1300]\tvalid_0's binary_logloss: 0.151592\tvalid_0's normalized_gini_coef: 0.287313\n[1400]\tvalid_0's binary_logloss: 0.151593\tvalid_0's normalized_gini_coef: 0.287344\nEarly stopping, best iteration is:\n[1264]\tvalid_0's binary_logloss: 0.151586\tvalid_0's normalized_gini_coef: 0.287481\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"LightGBM CV Gini Normalized Score: {np.mean(metric_evaluations):0.3f}({np.std(metric_evaluations):0.3f})\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:44:43.301245Z","iopub.execute_input":"2024-06-17T16:44:43.301677Z","iopub.status.idle":"2024-06-17T16:44:43.311317Z","shell.execute_reply.started":"2024-06-17T16:44:43.301640Z","shell.execute_reply":"2024-06-17T16:44:43.310326Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"LightGBM CV Gini Normalized Score: 0.289(0.015)\n","output_type":"stream"}]},{"cell_type":"code","source":"def batch_generator(X, batch_size, shuffle=True, random_state=None):\n    batch_index = 0\n    n = X.shape[0]\n    while True:\n        if batch_index == 0:\n            index_array = np.arange(n)\n            if shuffle:\n                np.random.seed(seed=random_state)\n                index_array = np.random.permutation(n)\n                \n        current_index = (batch_index * batch_size) % n\n        if n >= current_index + batch_size:\n            current_batxh_size = batch_size\n            batch_index += 1\n        else:\n            current_batch_size = n - current_index\n            batch_size = 0\n            \n        batch = X[index_array[current_index: current_index + current_batch_size]]\n        \n        yield batch","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:16:26.267370Z","iopub.status.idle":"2024-06-17T16:16:26.267764Z","shell.execute_reply.started":"2024-06-17T16:16:26.267582Z","shell.execute_reply":"2024-06-17T16:16:26.267599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"target\"] = preds\nsubmission.to_csv(\"lgb_submission.csv\")\n\noofs = target.to_frame()\noofs[\"target\"] = oof\noofs.to_csv(\"lgb_oof.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:46:49.478772Z","iopub.execute_input":"2024-06-17T16:46:49.479158Z","iopub.status.idle":"2024-06-17T16:46:52.730852Z","shell.execute_reply.started":"2024-06-17T16:46:49.479127Z","shell.execute_reply":"2024-06-17T16:46:52.729791Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def mixup_generator(X, batch_size, swaprate=0.15, shuffle=True, random_state=None):\n    if random_state is None:\n        random_state = np.randint(0, 999)\n    \n    num_features = X.shape[1]\n    num_swaps = int(num_features * swaprate)\n    generator_a = batch_generator(X, batch_size, shuffle, random_state)\n    generator_b = batch_generator(X, batch_size, shuffle, random_state+1)\n    \n    while True:\n        batch = next(generator_a)\n        mixed_batch = batch.copy()\n        effective_batch_size = batch.shape[0]\n        alternative_batch = next(generator_b)\n        assert((batch != alternative_batch).any())\n        for i in range(effective_batch_size):\n            swap_idx = np.random.choice(num_features, num_swaps, replace=False)\n            mixed_batch[i, swap_idx] = alternative_batch[i, swap_idx]\n        yield (mixed_batch, batch)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:16:26.268949Z","iopub.status.idle":"2024-06-17T16:16:26.269285Z","shell.execute_reply.started":"2024-06-17T16:16:26.269116Z","shell.execute_reply":"2024-06-17T16:16:26.269129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_DAE(X, architecture=[1500, 1500, 1500]):\n    features = X.shape[1]\n    inputs = Input((features,))\n    for i, nodes in enumerate(architecture):\n        layer = Dense(nodes, activation=\"relu\", use_bias=False, name=f\"code_{i+1}\")\n        if i==0:\n            x = layer(inputs)\n        else:\n            x = layer(x)\n        x = BatchNormalization()(x)\n    outputs = Dense(features, activation=\"linear\")(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\", \"mae\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:16:26.270580Z","iopub.status.idle":"2024-06-17T16:16:26.270942Z","shell.execute_reply.started":"2024-06-17T16:16:26.270771Z","shell.execute_reply":"2024-06-17T16:16:26.270787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_dae_features(autoencoder, X, layers=[3]):\n    data = []\n    for layer in layers:\n        if layer==0:\n            data.append(X)\n        else:\n            get_layer_output = Model([autoencoder.layers[0].input],\n                                    [autoencoder.layers[layer].output])\n            layer_output = get_layer_output.predict(X, batch_size=128)\n            data.append(layer_output)\n    data = np.hstack(data)\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:16:26.271989Z","iopub.status.idle":"2024-06-17T16:16:26.272345Z","shell.execute_reply.started":"2024-06-17T16:16:26.272167Z","shell.execute_reply":"2024-06-17T16:16:26.272182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndef autoencoder_fitting(X_train, X_valid, filename=\"dae\", random_state=None, suppress_output=False):\n    if suppress_output:\n        verbose = 0\n    else:\n        verbose = 2\n        print(\"Fittiong a denoising autoencoder\")\n        \n    tf.random.set_seed(seed=random_state)\n    generator = mixup_generator(X_train, batch_size=config.dae_batch_size, swaprate=0.15, random_state=config.random_state)\n    dae = get_DAE(X_train, architecture=config.dae_architecture)\n    steps_per_epoch = np.ceil(X_train.shape[0] / config.dae_batch_size)\n    \n    early_stopping = EarlStopping(monitor=\"val_mse\",\n                                 mode=\"min\",\n                                 patience=5,\n                                 restore_best_weight=True,\n                                 verbose=0)\n    history = dae.fit(generator,\n                     steps_per_epoch=steps_per_epoch,\n                     epochs=config.dae_num_epoch,\n                     )\n                     \"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:16:26.274645Z","iopub.status.idle":"2024-06-17T16:16:26.275010Z","shell.execute_reply.started":"2024-06-17T16:16:26.274836Z","shell.execute_reply":"2024-06-17T16:16:26.274852Z"},"trusted":true},"execution_count":null,"outputs":[]}]}